{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371b215f",
   "metadata": {},
   "source": [
    "# An attempt at making some functions for feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0bd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a14613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                  Non-Null Count   Dtype         \n",
      "---  ------                                  --------------   -----         \n",
      " 0   Unnamed: 0                              100000 non-null  int64         \n",
      " 1   Time                                    100000 non-null  datetime64[ns]\n",
      " 2   Date                                    100000 non-null  object        \n",
      " 3   Exchange                                100000 non-null  object        \n",
      " 4   Symbol                                  100000 non-null  object        \n",
      " 5   Trade_Volume                            100000 non-null  int64         \n",
      " 6   Trade_Price                             100000 non-null  float64       \n",
      " 7   Sale_Condition                          100000 non-null  object        \n",
      " 8   Source_of_Trade                         100000 non-null  object        \n",
      " 9   Trade_Stop_Stock_Indicator              0 non-null       float64       \n",
      " 10  Trade_Correction_Indicator              100000 non-null  int64         \n",
      " 11  Sequence_Number                         100000 non-null  int64         \n",
      " 12  Trade_Id                                100000 non-null  int64         \n",
      " 13  Trade_Reporting_Facility                100000 non-null  object        \n",
      " 14  Participant_Timestamp                   100000 non-null  int64         \n",
      " 15  Trade_Reporting_Facility_TRF_Timestamp  0 non-null       float64       \n",
      " 16  Trade_Through_Exempt_Indicator          100000 non-null  int64         \n",
      " 17  YearMonth                               100000 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(3), int64(8), object(6)\n",
      "memory usage: 13.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('data/trades_AAPL_20230106-20230110.csv.gz', compression='gzip')\n",
    "\n",
    "data['Time']=pd.to_datetime(data['Time'])\n",
    "\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fe3269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   Time          100000 non-null  datetime64[ns]\n",
      " 1   Date          100000 non-null  object        \n",
      " 2   Symbol        100000 non-null  object        \n",
      " 3   Trade_Price   100000 non-null  float64       \n",
      " 4   Trade_Volume  100000 non-null  int64         \n",
      " 5   YearMonth     100000 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(2)\n",
      "memory usage: 4.6+ MB\n",
      "None\n",
      "                        Time        Date Symbol  Trade_Price  Trade_Volume  \\\n",
      "0 2023-01-06 04:00:00.007354  2023-01-06   AAPL       125.24            40   \n",
      "1 2023-01-06 04:00:00.009587  2023-01-06   AAPL       125.22             1   \n",
      "2 2023-01-06 04:00:00.014786  2023-01-06   AAPL       125.22             4   \n",
      "3 2023-01-06 04:00:00.016263  2023-01-06   AAPL       125.22             1   \n",
      "4 2023-01-06 04:00:00.019133  2023-01-06   AAPL       125.22             1   \n",
      "\n",
      "   YearMonth  \n",
      "0     202301  \n",
      "1     202301  \n",
      "2     202301  \n",
      "3     202301  \n",
      "4     202301  \n",
      "                            Time        Date Symbol  Trade_Price  \\\n",
      "99995 2023-01-09 15:59:49.025299  2023-01-09   AAPL       130.17   \n",
      "99996 2023-01-09 15:59:49.034349  2023-01-09   AAPL       130.18   \n",
      "99997 2023-01-09 15:59:49.978286  2023-01-09   AAPL       130.18   \n",
      "99998 2023-01-09 15:59:50.044209  2023-01-09   AAPL       130.18   \n",
      "99999 2023-01-09 15:59:50.103684  2023-01-09   AAPL       130.19   \n",
      "\n",
      "       Trade_Volume  YearMonth  \n",
      "99995           100     202301  \n",
      "99996            11     202301  \n",
      "99997           100     202301  \n",
      "99998           100     202301  \n",
      "99999            38     202301  \n"
     ]
    }
   ],
   "source": [
    "selected_columns = [\"Time\", \"Date\", \"Symbol\", \"Trade_Price\", \"Trade_Volume\", \"YearMonth\"]\n",
    "filtered_data = data[selected_columns]\n",
    "\n",
    "print(filtered_data.info())\n",
    "print(filtered_data.head())\n",
    "print(filtered_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1423c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'filtered_data' is your DataFrame with the raw high-frequency trading data.\n",
    "\n",
    "# Function to find and return duplicate DateTime entries in the data\n",
    "def find_duplicates(df):\n",
    "    \"\"\"\n",
    "    Identify and return a DataFrame containing duplicate entries based on DateTime.\n",
    "\n",
    "    :param df: DataFrame with high-frequency trading data.\n",
    "    :return: DataFrame with duplicate DateTime entries.\n",
    "    \"\"\"\n",
    "    # Convert 'Time' and 'Date' to a single datetime column\n",
    "    # df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "    \n",
    "    # Check for duplicates in the DateTime column\n",
    "    duplicate_rows = df[df.duplicated('Time', keep=False)]\n",
    "    \n",
    "    return duplicate_rows\n",
    "\n",
    "# Call the function to find duplicates\n",
    "duplicate_entries = find_duplicates(filtered_data)\n",
    "\n",
    "print(duplicate_entries.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f455b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'filtered_data' is your DataFrame with the raw high-frequency trading data.\n",
    "\n",
    "# Function to resample DataFrame to 1-minute intervals\n",
    "def resample_data(df, resample_rate='1T', forward_fill=True):\n",
    "    \"\"\"\n",
    "    Resample the time series data to a specified rate.\n",
    "\n",
    "    :param df: DataFrame with high-frequency trading data.\n",
    "    :param resample_rate: The frequency rate to resample the DataFrame. Default is '1T' (1 minute).\n",
    "    :return: Resampled DataFrame with the original columns.\n",
    "    \"\"\"\n",
    "    # Make a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Convert 'Time' and 'Date' to a single datetime column and set as index\n",
    "    df_copy.set_index('Time', inplace=True)\n",
    "    \n",
    "    # Sort the index just in case it's not sorted\n",
    "    df_copy.sort_index(inplace=True)\n",
    "    \n",
    "    # Drop duplicate indices\n",
    "    df_copy = df_copy[~df_copy.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Define the aggregation dictionary for resampling\n",
    "    aggregation = {\n",
    "        'Trade_Price': 'last',\n",
    "        'Trade_Volume': 'sum'  # Assuming you want to sum the volumes within the interval\n",
    "    }\n",
    "    \n",
    "    # Resample the DataFrame\n",
    "    resampled_df = df_copy.resample(resample_rate).agg(aggregation)\n",
    "    \n",
    "    # Forward fill the Trade_Price to handle NaN values if there are no trades in the interval\n",
    "    if forward_fill: resampled_df['Trade_Price'].ffill(inplace=True)\n",
    "    \n",
    "    # Merge the resampled data back with the original dataframe to preserve non-numeric columns\n",
    "    # This aligns the non-numeric data with the resampled numeric data\n",
    "    df_non_numeric = df_copy[['Date', 'Symbol', 'YearMonth']].resample(resample_rate).ffill()\n",
    "    full_resampled_df = pd.concat([df_non_numeric, resampled_df], axis=1)\n",
    "    \n",
    "    return full_resampled_df\n",
    "\n",
    "# Call the function with the dataframe and the resampling rate\n",
    "# Example: resampled_dataframe = resample_data(filtered_data, '1T')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5573ae18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 5040 entries, 2023-01-06 04:00:00 to 2023-01-09 15:59:00\n",
      "Freq: T\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Date          5039 non-null   object \n",
      " 1   Symbol        5039 non-null   object \n",
      " 2   YearMonth     5039 non-null   float64\n",
      " 3   Trade_Price   5040 non-null   float64\n",
      " 4   Trade_Volume  5040 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 236.2+ KB\n",
      "None\n",
      "                           Date Symbol  YearMonth  Trade_Price  Trade_Volume\n",
      "Time                                                                        \n",
      "2023-01-06 04:00:00         NaN    NaN        NaN       125.34          3262\n",
      "2023-01-06 04:01:00  2023-01-06   AAPL   202301.0       125.33          3816\n",
      "2023-01-06 04:02:00  2023-01-06   AAPL   202301.0       125.21          2461\n",
      "2023-01-06 04:03:00  2023-01-06   AAPL   202301.0       125.23          1367\n",
      "2023-01-06 04:04:00  2023-01-06   AAPL   202301.0       125.11          6316\n",
      "                           Date Symbol  YearMonth  Trade_Price  Trade_Volume\n",
      "Time                                                                        \n",
      "2023-01-09 15:55:00  2023-01-09   AAPL   202301.0       129.96          6878\n",
      "2023-01-09 15:56:00  2023-01-09   AAPL   202301.0       130.05          2193\n",
      "2023-01-09 15:57:00  2023-01-09   AAPL   202301.0       130.09          1743\n",
      "2023-01-09 15:58:00  2023-01-09   AAPL   202301.0       130.09          4227\n",
      "2023-01-09 15:59:00  2023-01-09   AAPL   202301.0       130.19          7733\n"
     ]
    }
   ],
   "source": [
    "resampled_dataframe = resample_data(filtered_data, '1T')\n",
    "print(resampled_dataframe.info())\n",
    "print(resampled_dataframe.head())\n",
    "print(resampled_dataframe.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOL_LOOKBACK = 60  # for ex-ante volatility (in minutes)\n",
    "VOL_TARGET = 0.15  # 15% volatility target\n",
    "\n",
    "def calc_returns(srs: pd.Series, minute_offset: int = 1) -> pd.Series:\n",
    "    \"\"\"for each element of a pandas time-series srs,\n",
    "    calculates the returns over the past number of minutes\n",
    "    specified by offset\n",
    "\n",
    "    Args:\n",
    "        srs (pd.Series): time-series of prices\n",
    "        minute_offset (int, optional): number of minutes to calculate returns over. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: series of returns\n",
    "    \"\"\"\n",
    "    returns = srs / srs.shift(minute_offset) - 1.0\n",
    "    return returns\n",
    "\n",
    "def calc_minute_vol(minute_returns, lookback_minutes=60):\n",
    "    return (\n",
    "        minute_returns.ewm(span=lookback_minutes, min_periods=lookback_minutes)\n",
    "        .std()\n",
    "        .fillna(method=\"bfill\")\n",
    "    )\n",
    "\n",
    "def calc_normalised_returns(minute_offset, df_asset):\n",
    "        return (\n",
    "            calc_returns(df_asset[\"srs\"], minute_offset)\n",
    "            / df_asset[\"daily_vol\"]\n",
    "            / np.sqrt(minute_offset)\n",
    "        )\n",
    "\n",
    "# def calc_vol_scaled_returns(daily_returns, daily_vol=pd.Series(None)):\n",
    "#     \"\"\"calculates volatility scaled returns for annualised VOL_TARGET of 15%\n",
    "#     with input of pandas series daily_returns\"\"\"\n",
    "#     if not len(daily_vol):\n",
    "#         daily_vol = calc_minute_vol(daily_returns)\n",
    "#     annualised_vol = daily_vol * np.sqrt(252)  # annualised\n",
    "#     return daily_returns * VOL_TARGET / annualised_vol.shift(1)\n",
    "\n",
    "class MACDStrategy:\n",
    "    def __init__(self, trend_combinations: List[Tuple[float, float]] = None):\n",
    "        \"\"\"Used to calculated the combined MACD signal for a multiple short/signal combinations,\n",
    "        as described in https://arxiv.org/pdf/1904.04912.pdf\n",
    "\n",
    "        Args:\n",
    "            trend_combinations (List[Tuple[float, float]], optional): short/long trend combinations. Defaults to None.\n",
    "        \"\"\"\n",
    "        if trend_combinations is None:\n",
    "            self.trend_combinations = [(8, 24), (16, 48), (32, 96)]\n",
    "        else:\n",
    "            self.trend_combinations = trend_combinations\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_signal(srs: pd.Series, short_timescale: int, long_timescale: int) -> float:\n",
    "        \"\"\"Calculate MACD signal for a signal short/long timescale combination\n",
    "\n",
    "        Args:\n",
    "            srs ([type]): series of prices\n",
    "            short_timescale ([type]): short timescale\n",
    "            long_timescale ([type]): long timescale\n",
    "\n",
    "        Returns:\n",
    "            float: MACD signal\n",
    "        \"\"\"\n",
    "\n",
    "        def _calc_halflife(timescale):\n",
    "            return np.log(0.5) / np.log(1 - 1 / timescale)\n",
    "\n",
    "        macd = (\n",
    "            srs.ewm(halflife=_calc_halflife(short_timescale)).mean()\n",
    "            - srs.ewm(halflife=_calc_halflife(long_timescale)).mean()\n",
    "        )\n",
    "        q = macd / srs.rolling(63).std().fillna(method=\"bfill\")\n",
    "        return q / q.rolling(252).std().fillna(method=\"bfill\")\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_signal(y):\n",
    "        return y * np.exp(-(y ** 2) / 4) / 0.89\n",
    "\n",
    "    def calc_combined_signal(self, srs: pd.Series) -> float:\n",
    "        \"\"\"Combined MACD signal\n",
    "\n",
    "        Args:\n",
    "            srs (pd.Series): series of prices\n",
    "\n",
    "        Returns:\n",
    "            float: MACD combined signal\n",
    "        \"\"\"\n",
    "        return np.sum(\n",
    "            [self.calc_signal(srs, S, L) for S, L in self.trend_combinations]\n",
    "        ) / len(self.trend_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOL_THRESHOLD = 5  # multiple to winsorise by\n",
    "HALFLIFE_WINSORISE = 390 #number of minutes in a trading day. Not sure if this is an appropriate value or what\n",
    "\n",
    "# winsorize using rolling 5X standard deviations to remove outliers\n",
    "resampled_dataframe[\"srs\"] = resampled_dataframe[\"Trade_Price\"]\n",
    "ewm = resampled_dataframe[\"srs\"].ewm(halflife=HALFLIFE_WINSORISE)\n",
    "means = ewm.mean()\n",
    "stds = ewm.std()\n",
    "resampled_dataframe[\"srs\"] = np.minimum(resampled_dataframe[\"srs\"], means + VOL_THRESHOLD * stds)\n",
    "resampled_dataframe[\"srs\"] = np.maximum(resampled_dataframe[\"srs\"], means - VOL_THRESHOLD * stds)\n",
    "\n",
    "resampled_dataframe[\"minute_returns\"] = calc_returns(resampled_dataframe[\"srs\"])\n",
    "resampled_dataframe[\"minute_vol\"] = calc_minute_vol(resampled_dataframe[\"minute_returns\"])\n",
    "\n",
    "# vol scaling and shift to be next day returns\n",
    "# resampled_dataframe[\"target_returns\"] = calc_vol_scaled_returns(\n",
    "#     resampled_dataframe[\"daily_returns\"], resampled_dataframe[\"daily_vol\"]\n",
    "# ).shift(-1)\n",
    "\n",
    "def calc_normalised_returns(minute_offset):\n",
    "    return (\n",
    "        calc_returns(resampled_dataframe[\"srs\"], minute_offset)\n",
    "        / resampled_dataframe[\"minute_vol\"]\n",
    "        / np.sqrt(minute_offset)\n",
    "    )\n",
    "\n",
    "# intervals_in_minutes = [20, 60, 120, 180, 240]  # 20 minutes, 1 hour, 2 hours, 3 hours, 4 hours\n",
    "\n",
    "resampled_dataframe[\"norm_min_return\"] = calc_normalised_returns(1)\n",
    "resampled_dataframe[\"norm_20min_return\"] = calc_normalised_returns(20)\n",
    "resampled_dataframe[\"norm_60min_return\"] = calc_normalised_returns(60)\n",
    "resampled_dataframe[\"norm_120min_return\"] = calc_normalised_returns(120)\n",
    "resampled_dataframe[\"norm_180min_return\"] = calc_normalised_returns(180)\n",
    "\n",
    "trend_combinations = [(8, 24), (16, 48), (32, 96)]\n",
    "for short_window, long_window in trend_combinations:\n",
    "    resampled_dataframe[f\"macd_{short_window}_{long_window}\"] = MACDStrategy.calc_signal(\n",
    "        resampled_dataframe[\"srs\"], short_window, long_window\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resampled_dataframe.info())\n",
    "print(resampled_dataframe.head())\n",
    "print(resampled_dataframe.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Trade_Price and the winsorized srs series on the same plot. Trying to see differnece\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot original Trade_Price\n",
    "plt.plot(resampled_dataframe.index, resampled_dataframe[\"Trade_Price\"], label='Trade Price', color='blue')\n",
    "\n",
    "# Plot winsorized srs\n",
    "plt.plot(resampled_dataframe.index, resampled_dataframe[\"srs\"], label='Winsorized SRS', color='orange')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Trade Price vs Winsorized SRS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735453be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified function to calculate multiple MACD indicators based on arrays of window parameters\n",
    "\n",
    "def calculate_multiple_macds(df, short_windows, long_windows, signal_windows):\n",
    "    \"\"\"\n",
    "    Calculate multiple MACD indicators for each time step based on arrays of window sizes.\n",
    "\n",
    "    :param df: DataFrame with high-frequency trading data and a datetime index\n",
    "    :param short_windows: Array of short-term EMA window sizes\n",
    "    :param long_windows: Array of long-term EMA window sizes\n",
    "    :param signal_windows: Array of signal line EMA window sizes\n",
    "    :return: DataFrame with multiple MACD and Signal lines\n",
    "    \"\"\"\n",
    "    # Initialize a DataFrame to store MACD values for each set of windows\n",
    "    macd_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Loop through all provided window sizes\n",
    "    for short_window, long_window, signal_window in zip(short_windows, long_windows, signal_windows):\n",
    "        # Resample the trade prices to 1-minute intervals using the last price in the minute\n",
    "        price = df['Trade_Price'].resample('1T').last().ffill()\n",
    "        \n",
    "        # Calculate the short-term and long-term EMAs of the trade prices\n",
    "        short_ema = price.ewm(span=short_window, min_periods=1, adjust=False).mean()\n",
    "        long_ema = price.ewm(span=long_window, min_periods=1, adjust=False).mean()\n",
    "        \n",
    "        # Calculate the MACD line and the Signal line\n",
    "        macd = short_ema - long_ema\n",
    "        # signal_line = macd.ewm(span=signal_window, min_periods=1, adjust=False).mean()\n",
    "        \n",
    "        # Store the MACD and Signal line values in the DataFrame using the window sizes as column names\n",
    "        macd_df[f'MACD_{short_window}_{long_window}'] = macd\n",
    "        # macd_df[f'Signal_{short_window}_{long_window}_{signal_window}'] = signal_line\n",
    "\n",
    "    return macd_df\n",
    "\n",
    "# Example usage:\n",
    "# short_windows = [8, 16, 32]\n",
    "# long_windows = [24, 28, 96]\n",
    "# signal_windows = [9, 9, 9]  # Assuming the signal line uses a 9-period EMA for all\n",
    "# macd_values = calculate_multiple_macds(filtered_data, short_windows, long_windows, signal_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb89e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_windows = [8, 16, 32]\n",
    "long_windows = [24, 28, 96]\n",
    "signal_windows = [9, 9, 9]  # Assuming the signal line uses a 9-period EMA for all\n",
    "macd_values = calculate_multiple_macds(resampled_dataframe, short_windows, long_windows, signal_windows)\n",
    "\n",
    "print(macd_values.info())\n",
    "print(macd_values.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df76661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate volatility-normalized returns for different time intervals\n",
    "def calculate_vol_normalized_returns(df, intervals):\n",
    "    \"\"\"\n",
    "    Calculate the volatility-normalized returns for different time intervals.\n",
    "\n",
    "    :param df: DataFrame with high-frequency trading data and a datetime index.\n",
    "    :param intervals: List of intervals in minutes for which to calculate the normalized returns.\n",
    "    :return: DataFrame with volatility-normalized returns for each interval.\n",
    "    \"\"\"\n",
    "    # Calculate log returns\n",
    "    df['Log_Returns'] = np.log(df['Trade_Price'] / df['Trade_Price'].shift(1))\n",
    "    \n",
    "    # Create a DataFrame to store normalized returns for each interval\n",
    "    normalized_returns_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Loop over the intervals\n",
    "    for interval in intervals:\n",
    "        # Calculate the rolling standard deviation (volatility) for the current interval\n",
    "        rolling_volatility = df['Log_Returns'].rolling(window=interval).std()\n",
    "        \n",
    "        # Volatility-normalize the log returns by dividing by the rolling volatility\n",
    "        # Note that the first 'interval - 1' values will be NaN due to the rolling calculation\n",
    "        normalized_returns = df['Log_Returns'] / rolling_volatility\n",
    "        \n",
    "        # Store the normalized returns in the DataFrame\n",
    "        normalized_returns_df[f'Normalized_Returns_{interval}min'] = normalized_returns\n",
    "    \n",
    "    # Drop the initial rows where the rolling volatility can't be calculated\n",
    "    normalized_returns_df.dropna(how='all', inplace=True)\n",
    "    \n",
    "    return normalized_returns_df\n",
    "\n",
    "# Example intervals in minutes that might correspond to different intraday time horizons\n",
    "# Since 1 minute is our smallest increment, I dont think it possible for the volatility normaled returns to be determined since std = 0\n",
    "# intervals_in_minutes = [1, 60, 180, 360, 720]  # 1 minute, 1 hour, 3 hours, 6 hours, 12 hours\n",
    "\n",
    "# Assuming 'filtered_data' has a datetime index and a 'Trade_Price' column\n",
    "# Calculate the volatility-normalized returns\n",
    "# vol_normalized_returns = calculate_vol_normalized_returns(filtered_data, intervals_in_minutes)\n",
    "# print(vol_normalized_returns.head())\n",
    "\n",
    "# Note: This code assumes that 'filtered_data' is already sorted by datetime and has no duplicate indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61591ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example intervals in minutes that might correspond to different intraday time horizons\n",
    "intervals_in_minutes = [20, 60, 120, 180, 240]  # 20 minutes, 1 hour, 2 hours, 3 hours, 4 hours\n",
    "\n",
    "# Assuming 'filtered_data' has a datetime index and a 'Trade_Price' column\n",
    "# Calculate the volatility-normalized returns\n",
    "vol_normalized_returns = calculate_vol_normalized_returns(resampled_dataframe, intervals_in_minutes)\n",
    "print(vol_normalized_returns.info())\n",
    "print(vol_normalized_returns.head(15))\n",
    "\n",
    "# Note: This code assumes that 'filtered_data' is already sorted by datetime and has no duplicate indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86099df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate draw-ups and draw-downs at different frequencies\n",
    "def calculate_drawdowns_drawups(df, intervals):\n",
    "    \"\"\"\n",
    "    Calculate draw-ups and draw-downs based on highest and lowest prices over different past intervals.\n",
    "\n",
    "    :param df: DataFrame with high-frequency trading data and a datetime index.\n",
    "    :param intervals: List of intervals in minutes for which to calculate draw-ups and draw-downs.\n",
    "    :return: DataFrame with draw-ups and draw-downs for each interval.\n",
    "    \"\"\"\n",
    "    # Calculate log prices\n",
    "    df['Log_Price'] = np.log(df['Trade_Price'])\n",
    "    print(df['Log_Price'].head())\n",
    "    # Create a DataFrame to store drawdowns and draw-ups for each interval\n",
    "    features_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Loop over the intervals\n",
    "    for interval in intervals:\n",
    "        # Calculate the rolling max and min log prices for the current interval\n",
    "        rolling_max = df['Log_Price'].rolling(window=interval).max()\n",
    "        rolling_min = df['Log_Price'].rolling(window=interval).min()\n",
    "\n",
    "        print(rolling_max.head())\n",
    "        print(rolling_min.head())\n",
    "\n",
    "        # Calculate drawdowns and draw-ups\n",
    "        drawdowns = rolling_max - df['Log_Price']\n",
    "        drawups = df['Log_Price'] - rolling_min\n",
    "\n",
    "        # Store the drawdowns and draw-ups in the DataFrame\n",
    "        features_df[f'Drawdowns_{interval}min'] = drawdowns\n",
    "        features_df[f'Drawups_{interval}min'] = drawups\n",
    "\n",
    "    # Drop the initial rows where the rolling max and min can't be calculated\n",
    "    features_df.dropna(how='all', inplace=True)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "# Example intervals in minutes that might correspond to different intraday time horizons\n",
    "# intervals_in_minutes = [30, 60, 180, 360, 720]  # 1 minute, 1 hour, 3 hours, 6 hours, 12 hours\n",
    "\n",
    "# Assuming 'filtered_data' has a datetime index and a 'Trade_Price' column\n",
    "# Calculate the draw-ups and draw-downs\n",
    "# draw_features = calculate_drawdowns_drawups(filtered_data, intervals_in_minutes)\n",
    "# print(draw_features.head())\n",
    "\n",
    "# Note: This code assumes that 'filtered_data' is already sorted by datetime and has no duplicate indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e707c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_features = calculate_drawdowns_drawups(resampled_dataframe, intervals_in_minutes)\n",
    "draw_features = calculate_drawdowns_drawups(resampled_dataframe, [60])\n",
    "print(draw_features.head())\n",
    "\n",
    "# Note: This code assumes that 'filtered_data' is already sorted by datetime and has no duplicate indices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "query_user",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
